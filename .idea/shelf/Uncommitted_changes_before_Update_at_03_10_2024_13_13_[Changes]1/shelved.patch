Index: app/core/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pathlib\r\n\r\nfrom starlette.config import Config\r\n\r\nROOT = pathlib.Path(__file__).resolve().parent.parent  # app/\r\nBASE_DIR = ROOT.parent  # ./\r\n\r\nconfig = Config(BASE_DIR / \".env\")\r\n\r\n\r\nAPI_USERNAME = config(\"API_USERNAME\", str)\r\nAPI_PASSWORD = config(\"API_PASSWORD\", str)\r\n\r\n# Auth configs.\r\nAPI_SECRET_KEY = config(\"API_SECRET_KEY\", str)\r\nAPI_ALGORITHM = config(\"API_ALGORITHM\", str)\r\nAPI_ACCESS_TOKEN_EXPIRE_MINUTES = config(\r\n    \"API_ACCESS_TOKEN_EXPIRE_MINUTES\", int\r\n)  # infinity\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/core/config.py b/app/core/config.py
--- a/app/core/config.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ b/app/core/config.py	(date 1727697208340)
@@ -1,6 +1,7 @@
 import pathlib
 
 from starlette.config import Config
+from starlette.datastructures import CommaSeparatedStrings
 
 ROOT = pathlib.Path(__file__).resolve().parent.parent  # app/
 BASE_DIR = ROOT.parent  # ./
@@ -17,3 +18,41 @@
 API_ACCESS_TOKEN_EXPIRE_MINUTES = config(
     "API_ACCESS_TOKEN_EXPIRE_MINUTES", int
 )  # infinity
+
+DB_HOST = config("DATABASE_SERVER", str)
+EMBEDDINGS_SERVER = config("EMBEDDINGS_SERVER",str)
+OLLAMA_SERVER = config("OLLAMA_SERVER", str)
+AUDIO_TRANSCRIBE_SERVER = config("AUDIO_TRANSCRIBE_SERVER",str)
+AUDIO_PATH = config("AUDIO_PATH",str)
+
+INDEX_SETTINGS = {
+        "settings": {
+            "number_of_shards": 1,
+            "number_of_replicas": 0
+        },
+        "mappings": {
+            "properties": {
+                "name": {
+                    "type": "text",
+                    "analyzer": "standard"
+                },
+                "description": {
+                    "type": "text",
+                    "analyzer": "standard"
+                },
+                "user": {
+                    "type": "keyword"
+                }
+            }
+        }
+    }
+
+PROJECTS_INDEX = "projects_index"
+
+LDAP_ou = config("LDAP_ou",str)
+LDAP_dc = config("LDAP_dc",str)
+LDAP_server = config("LDAP_server",str)
+ADMIN_USERS = config("ADMIN_USERS", cast=CommaSeparatedStrings)
+
+LLM_MODEL = config("LLM_MODEL",str)
+EMBEDDINGS_MODEL = config("TRANSFORMER",str)
\ No newline at end of file
Index: app/apis/api_a/submod.py
===================================================================
diff --git a/app/apis/api_a/submod.py b/app/apis/api_a/submod.py
deleted file mode 100644
--- a/app/apis/api_a/submod.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ /dev/null	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
@@ -1,16 +0,0 @@
-# This a dummy module
-# This gets called in the module_main.py file
-
-from __future__ import annotations
-
-import random
-
-
-def rand_gen(num: int) -> dict[str, int]:
-    num = int(num)
-    d = {
-        "seed": num,
-        "random_first": random.randint(0, num),
-        "random_second": random.randint(0, num),
-    }
-    return d
Index: app/apis/api_b/mainmod.py
===================================================================
diff --git a/app/apis/api_b/mainmod.py b/app/apis/api_b/mainmod.py
deleted file mode 100644
--- a/app/apis/api_b/mainmod.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ /dev/null	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
@@ -1,8 +0,0 @@
-from __future__ import annotations
-
-from .submod import rand_gen
-
-
-def main_func(num: int) -> dict[str, int]:
-    d = rand_gen(num)
-    return d
Index: app/apis/api_b/submod.py
===================================================================
diff --git a/app/apis/api_b/submod.py b/app/apis/api_b/submod.py
deleted file mode 100644
--- a/app/apis/api_b/submod.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ /dev/null	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
@@ -1,16 +0,0 @@
-# This a dummy module
-# This gets called in the module_main.py file
-
-from __future__ import annotations
-
-import random
-
-
-def rand_gen(num: int) -> dict[str, int]:
-    num = int(num)
-    d = {
-        "seed": num,
-        "random_first": random.randint(0, num),
-        "random_second": random.randint(0, num),
-    }
-    return d
Index: app/apis/api_a/mainmod.py
===================================================================
diff --git a/app/apis/api_a/mainmod.py b/app/apis/api_a/mainmod.py
deleted file mode 100644
--- a/app/apis/api_a/mainmod.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ /dev/null	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
@@ -1,8 +0,0 @@
-from __future__ import annotations
-
-from .submod import rand_gen
-
-
-def main_func(num: int) -> dict[str, int]:
-    d = rand_gen(num)
-    return d
Index: .env
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># ENV\r\n\r\n# Host and port.\r\nHOST=\"0.0.0.0\" # localhost\r\nPORT=\"5002\"    # port to access the app\r\nPYTHON_VERSION=\"312\" # which dockerfile to use. see in dockerfiles/python*/Dockerfile\r\n\r\n# App config.\r\n\r\nAPI_USERNAME=\"ubuntu\"\r\nAPI_PASSWORD=\"debian\"\r\n\r\n# To get a string like this run:\r\n# openssl rand -hex 32\r\nAPI_SECRET_KEY=\"09d25e094faa6ca2556c818166b7a9563b93f7099f6f0f4caa6cf63b88e8d3e7\"\r\nAPI_ALGORITHM=\"HS256\"\r\n# infinity\r\nAPI_ACCESS_TOKEN_EXPIRE_MINUTES=\"5256000000\"\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.env b/.env
--- a/.env	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ b/.env	(date 1727696688661)
@@ -3,7 +3,7 @@
 # Host and port.
 HOST="0.0.0.0" # localhost
 PORT="5002"    # port to access the app
-PYTHON_VERSION="312" # which dockerfile to use. see in dockerfiles/python*/Dockerfile
+PYTHON_VERSION="310" # which dockerfile to use. see in dockerfiles/python*/Dockerfile
 
 # App config.
 
@@ -16,3 +16,20 @@
 API_ALGORITHM="HS256"
 # infinity
 API_ACCESS_TOKEN_EXPIRE_MINUTES="5256000000"
+
+LLM_MODEL = "llama3.1:8b-instruct-fp16"
+TRANSFORMER = "intfloat/multilingual-e5-large-instruct"
+
+#DATABASE SETTINGS
+EMBEDDINGS_SERVER = "http://192.168.53.58:7997/"
+DATABASE_SERVER = "http://192.168.53.58:9200"
+OLLAMA_SERVER = "http://192.168.53.58:11434/"
+AUDIO_TRANSCRIBE_SERVER = "http://192.168.53.58:8080/"
+AUDIO_PATH = "/home/administrador/audio2"
+
+#LDAP
+LDAP_ou = "Users"
+LDAP_dc = "iter,es"
+LDAP_server = "ldap://192.168.22.192:389"
+
+ADMIN_USERS = jfernandez,mpadron
\ No newline at end of file
Index: app/routes/views_tasks.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/routes/views_tasks.py b/app/routes/views_tasks.py
new file mode 100644
--- /dev/null	(date 1727878865948)
+++ b/app/routes/views_tasks.py	(date 1727878865948)
@@ -0,0 +1,163 @@
+from __future__ import annotations
+
+from fastapi import APIRouter, HTTPException
+from haystack.components.builders import PromptBuilder
+from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore
+from openai import OpenAI
+from starlette.responses import PlainTextResponse
+
+from app.apis.api_llm.llm_utils import TEMPLATE_SUMMARY, TEMPLATE_TRANSLATE
+from app.core.auth import decode_access_token
+from app.core.config import DB_HOST, OLLAMA_SERVER, LLM_MODEL
+
+router = APIRouter()
+
+@router.get("/agents/summarize/{token}/{file_name}/", tags=["agents"], response_class=PlainTextResponse)
+def get_summary(file_name: str, token:str):
+    """
+    Devuelve un resumen que captura los puntos mÃ¡s importantes.
+    """
+    index = decode_access_token(token)
+    print(index)
+    filters = {"field": "name", "operator": "==", "value": file_name.strip()}
+    document_store = ElasticsearchDocumentStore(hosts=DB_HOST, index="semantic_search") #TODO: Replace index
+    prediction = document_store.filter_documents(filters=filters)
+    document_store.client.close()
+
+    builder = PromptBuilder(template=TEMPLATE_SUMMARY)
+
+    #options = {"num_predict": -1, "temperature": 0.1}
+
+    if len(prediction) > 0:
+        orderedlist = sorted(prediction, key=lambda doc: doc.meta['paragraph'])  # order by paragraph
+        custom_docs = ""
+        current_speaker = ""
+        response = []
+        client = OpenAI(base_url=OLLAMA_SERVER + 'v1/', api_key='ollama', )
+        for doc in orderedlist:
+            if doc.meta["file_type"].startswith("audio/"):
+                if doc.meta['speaker'] != current_speaker:
+                    if current_speaker != "" and not custom_docs.strip().endswith((".", ",", "!", "?", ";")):
+                        custom_docs = custom_docs.strip() + "..."
+                    custom_docs += f"\n\n- {doc.meta['speaker']}: "
+                    current_speaker = doc.meta['speaker']
+            else:
+                if not custom_docs.strip().endswith((".", ",", "!", "?", ";")) and doc.meta['paragraph'] != 0:
+                    custom_docs = custom_docs.strip() + "..."
+                custom_docs += "\n\n"
+
+            custom_docs += f"{doc.content} "
+
+            if len(custom_docs) > 3500:
+                if doc.meta["file_type"].startswith("audio/"):
+                    current_speaker = ""
+                my_prompt = builder.run(myDocs=custom_docs)["prompt"].strip()
+                # print(my_prompt)
+                custom_docs = ""
+                #LLM CALL
+                completion = client.completions.create(
+                    model=LLM_MODEL,
+                    prompt=my_prompt,
+                    temperature=0.1,
+                    max_tokens=-1,
+                    stream=False
+                )
+                response.append("\n\n" + completion.choices[0].text)
+
+        if not custom_docs.strip().endswith((".", ",", "!", "?", ";")):
+            custom_docs = custom_docs.strip() + "."
+        if len(custom_docs) > 0:
+            my_prompt = builder.run(myDocs=custom_docs)["prompt"].strip()
+            custom_docs = ""
+            completion = client.completions.create(
+                model=LLM_MODEL,
+                prompt=my_prompt,
+                temperature=0.1,
+                max_tokens=-1,
+                stream=False
+            )
+            #if agent_response is None:
+            #    return ""
+            response.append("\n\n" + completion.choices[0].text)
+
+        return ''.join(response)
+    else:
+        raise HTTPException(status_code=404, detail="File not found!")
+
+
+@router.get("/audio/get_SRT_translated/{token}/{file_name}/{lang}/", tags=["audio"], response_class=PlainTextResponse)
+def get_SRT_traslated(file_name: str, lang: str, token:str):
+    """
+    Get the srt file referred to a specific audio file that have been processed previously, translated into a defined language. The language param admit most commons languages.
+    Ej: *EspaÃ±ol, inglÃ©s, alemÃ¡n, italiano, francÃ©s,...*
+    """
+    index = decode_access_token(token)
+    print(index)
+    document_store = ElasticsearchDocumentStore(hosts=DB_HOST, index="semantic_search")  # TODO: Replace index
+    filters = {"field": "name", "operator": "==", "value": file_name.strip()}
+    prediction = document_store.filter_documents(filters=filters)
+    document_store.client.close()
+    if len(prediction) > 0:
+        orderedlist = sorted(prediction, key=lambda d: d.meta['paragraph'])  # order by paragraph
+        # print("total docs",len(orderedlist))
+
+        builder = PromptBuilder(template=TEMPLATE_TRANSLATE)
+        #options = {"num_predict": -1, "temperature": 0.1, "top_p": 0.5}
+
+        prompt_list = []
+        srt_file = ""
+        count = 0
+        for i, doc in enumerate(orderedlist):
+            if doc.meta["file_type"].startswith("audio/"):
+                my_doc_mod = doc.content
+                srt_file += f"{str(i)}- {my_doc_mod.capitalize()}\n\n"
+                count += len(doc.content)
+                if count >= 1500:  # 6000 chars is aprox 1500 tokens
+                    my_prompt = builder.run(lang=lang, srt_file=srt_file)["prompt"].strip()
+                    prompt_list.append(my_prompt)
+                    srt_file = ""
+                    count = 0
+
+        if len(srt_file) > 0:
+            my_prompt = builder.run(lang=lang, srt_file=srt_file)["prompt"].strip()
+            prompt_list.append(my_prompt)
+
+        srt_file = []
+        client = OpenAI(base_url=OLLAMA_SERVER + 'v1/', api_key='ollama', )
+        for prompt in prompt_list:
+            #agent_result = launchAgent(prompt, options)
+            completion = client.completions.create(
+                model=LLM_MODEL,
+                prompt=prompt,
+                temperature=0.1,
+                max_tokens=-1,
+                top_p=0.5,
+                stream=False
+            )
+
+            data = completion.choices[0].text.split("\n")
+            # print(data)
+            final_list = [i for i in data if i]
+            for item in final_list[1:]:
+                x = item.find("-")
+                if x >= 1 and (x + 1) < len(item):
+                    srt_file.append(item[(x + 1):].strip())
+
+        #        print("total traslated paragraphs",len(srt_file))
+        plain_res = ""
+        for i, doc in enumerate(orderedlist):
+            if doc.meta["file_type"].startswith("audio/"):
+                plain_res += f"{str(i + 1)}\n"
+                text1 = f"{doc.meta['start_time']} --> {doc.meta['end_time']}\n"
+                plain_res += text1
+                text1 = f"{doc.meta['speaker']}: {doc.content}\n"
+                plain_res += text1
+                if i < len(srt_file):
+                    text1 = f"{doc.meta['speaker']}: {srt_file[i]}\n\n"
+                else:
+                    text1 = f"{doc.meta['speaker']}: --\n\n"
+                plain_res += text1
+
+        return plain_res
+    else:
+        raise HTTPException(status_code=404, detail="File not found!")
\ No newline at end of file
Index: app/routes/views_upload.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/routes/views_upload.py b/app/routes/views_upload.py
new file mode 100644
--- /dev/null	(date 1727790066253)
+++ b/app/routes/views_upload.py	(date 1727790066253)
@@ -0,0 +1,139 @@
+from __future__ import annotations
+
+import concurrent
+import copy
+import json
+import os
+import shutil
+from typing import Optional, Dict, Annotated, List
+
+from fastapi import APIRouter, Body, BackgroundTasks, UploadFile, File
+from pydantic import BaseModel, ConfigDict, Field, model_validator
+
+from app.apis.api_documents.write_docs import generateSRT_doc, document_manager, audio_manager, download_file
+from app.core.auth import decode_access_token
+from app.core.config import AUDIO_PATH
+
+
+class InputParams(BaseModel):
+    model_config = ConfigDict(extra='forbid')
+    token: str = Field(title="Access token", max_length=175)
+    metadata: Optional[List[dict,]] = None
+
+    @model_validator(mode='before')
+    @classmethod
+    def validate_to_json(cls, value):
+        if isinstance(value, str):
+            return cls(**json.loads(value))
+        return value
+
+class uploadURL(InputParams):
+    urls:List[str] = Field(title="URI")
+
+router = APIRouter()
+
+class plainSRTParams(BaseModel):
+    model_config = ConfigDict(extra='forbid')
+    text:str = Field(title="SRT text")
+    filename:str = Field(title="File name", max_length=150)
+    metadata:Optional[Dict] = None
+    token:str = Field(title="Access token", max_length=175)
+
+@router.post("/documents/upload/plainSRT/", tags=["documents"])
+def upload_plain_srt(input_values:plainSRTParams, background_tasks: BackgroundTasks) -> dict:
+    """
+    Accepts .srt files in plain text for processing.
+    File name required.
+    Metadata example for each file added: `{"user": ["some", "more"], "category": ["only_one"]}`    OR    empty
+    """
+
+    index = decode_access_token(input_values.token)
+    print(index)
+
+    background_tasks.add_task(generateSRT_doc, input_values.filename, input_values.text, input_values.metadata, index)
+    return {"message":"SRT plain text uploaded correctly, processing..." , "filenames":input_values.filename}
+
+
+@router.post("/documents/upload/", tags=["documents"])
+def upload_documents(files: Annotated[List[UploadFile], File(description="files")], background_tasks: BackgroundTasks,
+                     metadata: InputParams = Body(...)):
+    """
+    Accepts documents in **.doc, .pdf .xps, .epub, .mobi, .fb2, .cbz, .svg,.txt and .srt** format. It only can be parsed by paragraphs.
+
+    `TODO: Add support for other parsing options.`
+
+    Metadata example for each file added: `{"name": ["some", "more"], "category": ["only_one"]}`    OR    empty
+    """
+    index = decode_access_token(metadata.token)
+    print(index)
+    #
+    # https://stackoverflow.com/questions/63110848/how-do-i-send-list-of-dictionary-as-body-parameter-together-with-files-in-fastap
+    for file in files:
+        if file.content_type.startswith("audio/"):
+            return {
+                "message": "Error uploading files! This endpint only accepts text files. Use /audio/upload/ endpoint for uploading audio files"}
+    background_tasks.add_task(document_manager, copy.deepcopy(files), metadata.metadata, index)
+
+
+@router.post("/audio/upload/", tags=["audio"])
+def upload_audios(files: Annotated[List[UploadFile], File(description="files")], background_tasks: BackgroundTasks,
+                  metadata: InputParams = Body(...)):
+    """
+    This endpoint accepts audio in **.mp3** format. It only can be parsed by "\\n" character.
+
+    `TODO: Add support for other parsing options.`
+
+    Metadata example for each file added: `{"name": ["some", "more"], "category": ["only_one"]}`    OR    empty
+    """
+    index = decode_access_token(metadata.token)
+    print(index)
+    #
+    # https://stackoverflow.com/questions/63110848/how-do-i-send-list-of-dictionary-as-body-parameter-together-with-files-in-fastap
+    filenames = []
+    for file in files:
+        if file.content_type.startswith("audio/"):
+            audio_file = os.path.join(AUDIO_PATH, file.filename)
+            if os.path.isfile(audio_file):
+                print('File exists')
+            else:
+                try:
+                    with open(audio_file, "wb") as buffer:
+                        shutil.copyfileobj(file.file, buffer)
+                        filenames.append(audio_file)
+                except Exception:
+                    print("There was an error uploading the file")
+                finally:
+                    file.file.close()
+    # print(filenames)
+    background_tasks.add_task(audio_manager, filenames, metadata.metadata, index)
+
+    return {"message": "Files uploaded correctly, processing...",
+            "filenames": [os.path.basename(file) for file in filenames]}
+
+
+@router.post("/audio/upload/from_url", tags=["audio"])
+def upload_audio_url(files: uploadURL, background_tasks: BackgroundTasks):
+    """
+    This endpoint accepts documents in **.doc, .pdf .xps, .epub, .mobi, .fb2, .cbz, .svg,.txt and.srt** format from external URLs It only can be parsed by paragraphs.
+
+    `TODO: Add support for other parsing options.`
+
+    Metadata example for each file added: `{"name": ["some", "more"], "category": ["only_one"]}`    OR    empty
+    """
+    index = decode_access_token(files.token)
+    print(index)
+
+    filenames = []
+    if len(files.urls) > 0:
+        with concurrent.futures.ThreadPoolExecutor() as executor:
+            futures = []
+            for url in files.urls:
+                futures.append(executor.submit(download_file, url))
+            for future in concurrent.futures.as_completed(futures):
+                filenames.append(future.result())
+        filenames = [x for x in filenames if x]  # clear emptys
+        if len(filenames) > 0:
+            background_tasks.add_task(audio_manager, filenames, files.metadata)
+            return {"message": "Files uploaded correctly, processing...",
+                    "filenames": [os.path.basename(file) for file in filenames]}
+    return {"message": "No files processed!"}
\ No newline at end of file
Index: app/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from fastapi import FastAPI\r\nfrom starlette.middleware.cors import CORSMiddleware\r\n\r\nfrom app.core import auth\r\nfrom app.routes import views\r\n\r\napp = FastAPI()\r\n\r\n# Set all CORS enabled origins\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"*\"],\r\n    allow_credentials=True,\r\n    allow_methods=[\"*\"],\r\n    allow_headers=[\"*\"],\r\n)\r\n\r\napp.include_router(auth.router)\r\napp.include_router(views.router)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/main.py b/app/main.py
--- a/app/main.py	(revision c06e1f6afdb5a3cce83b97029b24eb80bc4be5b2)
+++ b/app/main.py	(date 1727788876879)
@@ -2,9 +2,59 @@
 from starlette.middleware.cors import CORSMiddleware
 
 from app.core import auth
-from app.routes import views
+from app.routes import views, views_upload, views_tasks
+
+description = """Ask questions about the documents provided. ðŸš€
+Model Max Sequence Length: **512**.
+Services required: 
+* infinity service (https://github.com/michaelfeil/infinity)
+* Elastic search service.
+* Ollama service.
+* Diarization service."""
 
-app = FastAPI()
+tags_metadata = [
+    {
+       "name": "projects",
+       "description": "Manage token's projects",
+    },
+    {
+        "name": "search",
+        "description": "Get answers about documents",
+        #"externalDocs": {
+        #    "description": "Made with FastApi",
+        #    "url": "https://fastapi.tiangolo.com/",
+        #},
+    },
+    {
+        "name": "documents",
+        "description": "Manage documents",
+    },
+    {
+        "name": "audio",
+        "description": "Manage audio",
+    },
+    {
+        "name": "agents",
+        "description": "Helpful tasks",
+    },
+]
+
+app = FastAPI(
+    title="Semantic search service",
+    description=description,
+    summary="Semantic search in documents",
+    version="3.0.0",
+    #terms_of_service="http://example.com/terms/",
+    contact={
+        "name": "Javier FernÃ¡ndez",
+        #"url": "http://iter.es/contact/",
+        "email": "jfernandez@iter.es",
+    },
+    license_info={
+        "name": "Apache 2.0",
+        "identifier": "MIT",
+    },openapi_tags=tags_metadata
+    )
 
 # Set all CORS enabled origins
 app.add_middleware(
@@ -17,3 +67,5 @@
 
 app.include_router(auth.router)
 app.include_router(views.router)
+app.include_router(views_upload.router)
+app.include_router(views_tasks.router)
Index: app/apis/api_documents/write_docs.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/apis/api_documents/write_docs.py b/app/apis/api_documents/write_docs.py
new file mode 100644
--- /dev/null	(date 1727790638148)
+++ b/app/apis/api_documents/write_docs.py	(date 1727790638148)
@@ -0,0 +1,156 @@
+from __future__ import annotations
+
+import concurrent
+import os
+import shutil
+import time
+from datetime import datetime
+
+import requests
+import validators
+from haystack import Document
+from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore
+
+from app.apis.api_documents.ProcessText import ProcessText
+from app.core.config import EMBEDDINGS_SERVER, EMBEDDINGS_MODEL, DB_HOST, AUDIO_TRANSCRIBE_SERVER, AUDIO_PATH
+
+
+def document_manager(files, metadata, index:str):
+    start = time.process_time()
+    with concurrent.futures.ThreadPoolExecutor() as executor:  # optimally defined number of threads
+        for i, file in enumerate(files):
+            nameParsed = file.filename.split(".")
+            if len(nameParsed) > 1:
+                meta = None
+                if metadata and len(metadata) > i:
+                    meta = metadata[i]
+                executor.submit(processFile, file, nameParsed[1], meta, index)
+                # processFile(file,nameParsed[1], meta)
+            else:
+                print("error extension1")
+
+    end = time.process_time()
+    print("Processing time:", end - start)
+
+
+def audio_manager(files, metadata, index:str):
+    if len(files) > 0:
+        try:
+            req = requests.post(AUDIO_TRANSCRIBE_SERVER + "transcribe/", json={"audio_path": files})  #transcribe audio files
+            req.raise_for_status()
+            #print(req.json())
+        except requests.exceptions.RequestException as e:
+            print("Error!", str(e))
+            #return "Error!"
+
+
+def processFile(file, ext, metadata, index:str):
+    texts, pages = ProcessText.readFile(file.file.read(),ext)
+    if ext == ProcessText.SRT_FORMAT:
+        generateSRT_doc(file.filename, texts[0], metadata, index)
+    else:
+        generate_doc(file, texts, metadata, pages, index)
+
+
+#plain text srt to dict
+def parseSRT(texts:str) -> []:
+    splitted_srt = texts.strip().split("\n\n")
+    splitted_srt = [x for x in splitted_srt if x.strip()] #remove whites
+    current_texts = []
+    #split srt file
+    for i, chunk in enumerate(splitted_srt, start=1):
+        splitted_chunk = chunk.split("\n")
+        if len(splitted_chunk) > 2:
+            full_time = splitted_chunk[1].split("-->")
+            start_time = full_time[0].strip()
+            end_time = full_time[1].strip()
+            content = ""
+            for text_chunk in splitted_chunk[2:]:
+                content += text_chunk + " "
+            index_char = content.find(":")
+            speaker = ""
+            if 0 < index_char < 30 and len(content) > 0 and len(content) > (index_char + 1):
+                speaker = content[0:index_char].strip()
+                content = content[(index_char + 1):]
+            current_texts.append({"content":content.strip(), "metadata":{"paragraph":i, "start_time":start_time, "end_time":end_time, "speaker":speaker}})
+
+    return current_texts
+
+
+def generateSRT_doc(filename:str, srt_text:str, metadata:dict, index:str):
+    # print(filename,metadata)
+    srt_json = parseSRT(srt_text)
+    docs = []
+
+    current_texts = []
+    for audio_chunk in srt_json:
+        current_texts.append(audio_chunk["content"].replace("\"", "\'"))
+
+    if len(current_texts) > 0:
+        try:
+            req = requests.post(EMBEDDINGS_SERVER + "embeddings",
+                                json={"input": current_texts, "model": EMBEDDINGS_MODEL})  # encode texts infinity api
+            if req.status_code == 200:
+                doc_emb = req.json()["data"]
+                now = datetime.now()
+                dt_string = now.strftime("%d/%m/%Y %H:%M:%S")
+                for i, text in enumerate(current_texts):
+                    currentMetadata = srt_json[i]["metadata"]
+
+                    currentMetadata.update(metadata)
+                    currentMetadata.update({"timestamp": dt_string, "page": 1, "name": filename, "file_type": "audio/"})
+                    docs.append(Document(content=text, meta=currentMetadata, embedding=doc_emb[i]["embedding"]))
+        except requests.exceptions.RequestException as e:
+            print("ERROR!")
+    if len(docs) > 0:
+        document_store = ElasticsearchDocumentStore(hosts=DB_HOST, index="semantic_search")  # TODO: Replace index
+        document_store.write_documents(docs)
+        document_store.client.close()
+
+
+def generate_doc(file, texts, metadata, pages, index:str):
+    docs = []
+    if len(texts) > 0:
+        try:
+            req = requests.post(EMBEDDINGS_SERVER + "embeddings/",
+                                json={"input": texts, "model": EMBEDDINGS_MODEL})  # encode texts infinity api
+            if req.status_code == 200:
+                doc_emb = req.json()["data"]
+                now = datetime.now()
+                dt_string = now.strftime("%d/%m/%Y %H:%M:%S")
+                for i, text in enumerate(texts):
+                    currentMetadata = {"name": file.filename, "paragraph": i, "timestamp": dt_string,
+                                       "file_type": file.content_type}
+                    if metadata:
+                        currentMetadata.update(metadata)
+                    if len(pages) == len(texts):
+                        currentMetadata.update({"page": pages[i]})
+                    elif len(pages) == 1:
+                        currentMetadata.update({"page": pages[0]})
+
+                    docs.append(Document(content=text.replace("\"", "\'"), meta=currentMetadata,
+                                         embedding=doc_emb[i]["embedding"]))
+        except requests.exceptions.RequestException as e:
+            print("ERROR!")
+            # TODO set logging error
+    if len(docs) > 0:
+        document_store = ElasticsearchDocumentStore(hosts=DB_HOST, index="semantic_search")  # TODO: Replace index
+        document_store.write_documents(docs)
+        document_store.client.close()
+
+
+def download_file(url):
+    if validators.url(url):
+        filename = url.split("/")[-1]
+        audio_file = os.path.join(AUDIO_PATH, filename)
+        if not os.path.isfile(audio_file):
+            try:
+                with requests.get(url, stream=True, timeout=10) as r:
+                    with open(audio_file, 'wb') as f:
+                        shutil.copyfileobj(r.raw, f)
+                return audio_file
+            except requests.exceptions.RequestException as e:
+                print("Error uploading the file " + filename + ": " + str(e))
+            except Exception as err:
+                print("Error uploading the file " + filename + ": " + str(err))
+    return ""
\ No newline at end of file
Index: .idea/fastapi-nano.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/fastapi-nano.iml b/.idea/fastapi-nano.iml
new file mode 100644
--- /dev/null	(date 1727786423508)
+++ b/.idea/fastapi-nano.iml	(date 1727786423508)
@@ -0,0 +1,21 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$">
+      <excludeFolder url="file://$MODULE_DIR$/.venv" />
+    </content>
+    <orderEntry type="inheritedJdk" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+  <component name="PackageRequirementsSettings">
+    <option name="versionSpecifier" value="Don't specify version" />
+    <option name="removeUnused" value="true" />
+  </component>
+  <component name="PyDocumentationSettings">
+    <option name="format" value="PLAIN" />
+    <option name="myDocStringFormat" value="Plain" />
+  </component>
+  <component name="TestRunnerService">
+    <option name="PROJECT_TEST_RUNNER" value="py.test" />
+  </component>
+</module>
\ No newline at end of file
Index: app/apis/api_documents/ProcessText.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/apis/api_documents/ProcessText.py b/app/apis/api_documents/ProcessText.py
new file mode 100644
--- /dev/null	(date 1727791830790)
+++ b/app/apis/api_documents/ProcessText.py	(date 1727791830790)
@@ -0,0 +1,187 @@
+from __future__ import annotations
+
+import io
+import pymupdf
+from docx import Document as Document_Doc
+from thefuzz import fuzz
+
+
+# from lib.ProcessMyPDF import *
+
+class ProcessText:
+    VALID_FORMATS_FITZ = ["pdf", "xps", "epub", "mobi", "fb2", "cbz", "svg"]
+    VALID_FORMATS = ["txt"]
+    DOCS_FORMAT = ["docx"]
+    SRT_FORMAT = "srt"
+
+    @staticmethod
+    def check_if_header(pages):
+        if len(pages) > 1:
+            pages[0] = pages[0].strip()
+            first_page = pages[0].splitlines()
+            for idx, line in enumerate(first_page):
+                for s in pages[1:]:
+                    s = s.strip()
+                    next_page = s.splitlines()
+                    # print("line: "+str(idx) + "|* page0*|: " + line + " |*page"+ "*|: "+ next_page[idx])
+                    if len(next_page) <= idx or fuzz.ratio(next_page[idx], line) < 90:
+                        return idx
+        return 0
+
+    @staticmethod
+    def check_if_footer(pages):
+        if len(pages) > 1:
+            pages[0] = pages[0].strip()
+            first_page = pages[0].splitlines()
+            idx = 0
+            for line in reversed(first_page):
+                for s in pages[1:]:
+                    s = s.strip()
+                    next_page = s.splitlines()
+                    current_line = len(next_page) - idx - 1
+                    if current_line < 0 or fuzz.ratio(next_page[current_line], line) < 90:
+                        return idx
+                idx += 1
+        return 0
+
+    @staticmethod
+    def remove_header_footer(text):
+        """Remove header and footer"""
+        pages = text.split("\f")
+        # pages = [x for x in pages if len(x.strip())>0] #remove white slices
+        header = ProcessText.check_if_header(pages)
+        footer = ProcessText.check_if_footer(pages)
+        if header > 0 or footer > 0:
+            text_formatted = ""
+            for page in pages:
+                # text_formatted = text_formatted + remove_header_footer(page, header, footer)
+                page = page.strip()
+                text_splitted = page.splitlines()
+                result = ""
+                value = len(text_splitted)
+                if footer > 0:
+                    value = -1 * footer
+                for line in text_splitted[header:value]:
+                    text_formatted = text_formatted + line + "\r\n"
+                text_formatted = text_formatted + "\f\r\n"
+
+            return text_formatted.strip()
+        else:
+            return text
+
+    @staticmethod
+    def readFile(bytesFile, fileType):
+        txt_formatted = ""
+        splitted_text = []
+        text_pages = []
+        fileType = fileType.lower()
+        # bytesFile = file.file.read()
+        if fileType in ProcessText.VALID_FORMATS_FITZ:
+            plain_text = ProcessText.readBinaryFile(bytesFile, fileType)
+            txt_formatted = ProcessText.remove_header_footer(plain_text)
+            splitted_text, text_pages = ProcessText.chunk_text(txt_formatted)  # fix text
+
+        elif fileType in ProcessText.VALID_FORMATS:
+            plain_text = bytesFile.decode("utf-8").strip()
+            txt_formatted = ProcessText.remove_header_footer(plain_text)
+            splitted_text, text_pages = ProcessText.chunk_text(txt_formatted)  # fix text
+
+        elif fileType in ProcessText.SRT_FORMAT:
+            plain_text = bytesFile.decode("utf-8").strip()
+            text_pages.append(1)
+            splitted_text = [plain_text]
+
+        elif fileType in ProcessText.DOCS_FORMAT:
+            splitted_text = ProcessText.readFileDocx(bytesFile)
+            text_pages.append(1)  # docx hasn't got pages
+
+        else:
+            print("Error extension")
+            # return txt_formatted
+
+        # txt_formatted = ProcessText.remove_header_footer(plain_text)
+        return splitted_text, text_pages
+
+    @staticmethod
+    def readBinaryFile(bytesFile, myformat):
+        # TODO: https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517
+        doc = pymupdf.open(stream=bytesFile, filetype=myformat)
+        plain_text = ""
+        for page in doc:
+
+            # tables=page.find_tables()
+            # for tab in page.find_tables():
+            # # process the content of table 'tab'
+            # page.add_redact_annot(tab.bbox)  # wrap table in a redaction annotation
+
+            # page.apply_redactions()  # erase all table text
+            blocks = page.get_text("blocks", sort=True)
+            for block in blocks:
+                if block[6] == 0:  # 0 text, 1 image
+                    plain_text += block[4]  # 4 = text
+                    item = block[4].rstrip()
+                    if len(item) > 0 and item[-1] in [".", "?", "!"]:
+                        plain_text += "\r\n"
+            plain_text = plain_text + "\f\r\n"
+        return plain_text.strip()
+
+    @staticmethod
+    def readFileDocx(bytesFile):
+        source_stream = io.BytesIO(bytesFile)
+        mydocument = Document_Doc(source_stream)
+        source_stream.close()
+        # I = []
+        # plain_text = ""
+
+        # paragraphs = mydocument.paragraphs
+        paragraphs = [x.text.strip() for x in mydocument.paragraphs]
+        # for i in range(len(paragraphs)):
+        #    plain_text += paragraphs[i].text.strip()
+        #    plain_text += "\r\n"
+        # if 'graphicData' in par[i]._p.xml:
+        #    I.append(i)
+        # print(I)
+        # return (plain_text,I)
+        # return plain_text.strip()
+        return paragraphs
+
+    # @staticmethod
+    # def extract_Docx_tables(document):
+    # tables = []
+    # for table in document.tables:
+    # df = [['' for i in range(len(table.columns))] for j in range(len(table.rows))]
+    # for i, row in enumerate(table.rows):
+    # for j, cell in enumerate(row.cells):
+    # if cell.text:
+    # df[i][j] = cell.text
+    # tables.append(pd.DataFrame(df))
+    # return tables
+
+    @staticmethod
+    def chunk_text(input):
+        '''
+        parse text by empty line. clear emtpy lines and whites in the end.
+        '''
+        text = ""
+        splitted_text = []
+        text_page = []
+        buf = io.StringIO(input)
+        file_content = buf.readlines()
+        page_counter = 1
+        for line in file_content:
+            # line = line.decode("utf-8")
+            aux = line.strip()
+            if len(aux) == 0:  # breakline
+                if len(text) > 0 and text.strip().endswith("."):
+                    splitted_text.append(text.strip())
+                    text_page.append(page_counter)
+                    text = ""
+                if "\f" in line:  # end of page
+                    page_counter += 1
+                # continue
+            else:
+                if not aux.endswith(".") and not aux.endswith(":"):
+                    line = aux + " "
+                text = text + line
+
+        return splitted_text, text_page
Index: .idea/inspectionProfiles/profiles_settings.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml
new file mode 100644
--- /dev/null	(date 1727274620015)
+++ b/.idea/inspectionProfiles/profiles_settings.xml	(date 1727274620015)
@@ -0,0 +1,6 @@
+<component name="InspectionProjectProfileManager">
+  <settings>
+    <option name="USE_PROJECT_PROFILE" value="false" />
+    <version value="1.0" />
+  </settings>
+</component>
\ No newline at end of file
Index: .idea/modules.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/modules.xml b/.idea/modules.xml
new file mode 100644
--- /dev/null	(date 1727274619973)
+++ b/.idea/modules.xml	(date 1727274619973)
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/fastapi-nano.iml" filepath="$PROJECT_DIR$/.idea/fastapi-nano.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
new file mode 100644
--- /dev/null	(date 1727275800716)
+++ b/.idea/misc.xml	(date 1727275800716)
@@ -0,0 +1,10 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="Black">
+    <option name="sdkName" value="Python 3.10 (fastapi-nano)" />
+  </component>
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.10 (fastapi-nano)" project-jdk-type="Python SDK" />
+  <component name="PythonCompatibilityInspectionAdvertiser">
+    <option name="version" value="3" />
+  </component>
+</project>
\ No newline at end of file
